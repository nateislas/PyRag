"""Embedding service for PyRAG."""

from typing import List, Optional
import numpy as np
from sentence_transformers import SentenceTransformer
import torch

from .config import settings
from .logging import get_logger

logger = get_logger(__name__)


class EmbeddingService:
    """Service for generating text embeddings."""
    
    def __init__(self):
        """Initialize embedding service."""
        self.logger = get_logger(__name__)
        self.logger.info("Initializing embedding service")
        
        # Load embedding model
        self.model_name = settings.embeddings.model_name
        self.model = SentenceTransformer(self.model_name)
        
        # Get embedding dimensions
        self.embedding_dim = self.model.get_sentence_embedding_dimension()
        
        self.logger.info(f"Loaded embedding model: {self.model_name} (dim: {self.embedding_dim})")
    
    async def embed_text(self, text: str) -> List[float]:
        """Generate embedding for a single text."""
        try:
            # Generate embedding
            embedding = self.model.encode(text, convert_to_tensor=False)
            
            # Convert to list if it's a numpy array
            if isinstance(embedding, np.ndarray):
                embedding = embedding.tolist()
            
            return embedding
        except Exception as e:
            self.logger.error(f"Error generating embedding: {e}")
            # Return zero vector as fallback
            return [0.0] * self.embedding_dim
    
    async def embed_texts(self, texts: List[str]) -> List[List[float]]:
        """Generate embeddings for multiple texts."""
        try:
            # Generate embeddings in batch
            embeddings = self.model.encode(texts, convert_to_tensor=False)
            
            # Convert to list of lists
            if isinstance(embeddings, np.ndarray):
                embeddings = embeddings.tolist()
            
            return embeddings
        except Exception as e:
            self.logger.error(f"Error generating embeddings: {e}")
            # Return zero vectors as fallback
            return [[0.0] * self.embedding_dim for _ in texts]
    
    async def embed_query(self, query: str) -> List[float]:
        """Generate embedding for a search query."""
        return await self.embed_text(query)
    
    async def similarity(self, embedding1: List[float], embedding2: List[float]) -> float:
        """Calculate cosine similarity between two embeddings."""
        try:
            # Convert to numpy arrays
            vec1 = np.array(embedding1)
            vec2 = np.array(embedding2)
            
            # Calculate cosine similarity
            similarity = np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))
            
            return float(similarity)
        except Exception as e:
            self.logger.error(f"Error calculating similarity: {e}")
            return 0.0
    
    async def batch_similarity(
        self,
        query_embedding: List[float],
        document_embeddings: List[List[float]]
    ) -> List[float]:
        """Calculate similarities between query and multiple documents."""
        try:
            query_vec = np.array(query_embedding)
            doc_vecs = np.array(document_embeddings)
            
            # Calculate cosine similarities
            similarities = np.dot(doc_vecs, query_vec) / (
                np.linalg.norm(doc_vecs, axis=1) * np.linalg.norm(query_vec)
            )
            
            return similarities.tolist()
        except Exception as e:
            self.logger.error(f"Error calculating batch similarities: {e}")
            return [0.0] * len(document_embeddings)
    
    def get_embedding_dimension(self) -> int:
        """Get the dimension of embeddings generated by this model."""
        return self.embedding_dim
    
    async def health_check(self) -> bool:
        """Check if the embedding service is healthy."""
        try:
            # Test embedding generation
            test_text = "This is a test embedding."
            embedding = await self.embed_text(test_text)
            
            # Check if embedding has correct dimension
            if len(embedding) == self.embedding_dim:
                self.logger.info("Embedding service health check passed")
                return True
            else:
                self.logger.error(f"Embedding dimension mismatch: expected {self.embedding_dim}, got {len(embedding)}")
                return False
        except Exception as e:
            self.logger.error(f"Embedding service health check failed: {e}")
            return False
